{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-20T07:23:39.903840Z",
     "iopub.status.busy": "2025-04-20T07:23:39.903655Z",
     "iopub.status.idle": "2025-04-20T07:26:03.760717Z",
     "shell.execute_reply": "2025-04-20T07:26:03.759953Z",
     "shell.execute_reply.started": "2025-04-20T07:23:39.903824Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu124\n",
      "Loading datasets...\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./datasets/mnist_data/data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:00<00:00, 16.1MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./datasets/mnist_data/data/MNIST/raw/train-images-idx3-ubyte.gz to ./datasets/mnist_data/data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./datasets/mnist_data/data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 481kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./datasets/mnist_data/data/MNIST/raw/train-labels-idx1-ubyte.gz to ./datasets/mnist_data/data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./datasets/mnist_data/data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.46MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./datasets/mnist_data/data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./datasets/mnist_data/data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./datasets/mnist_data/data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 8.66MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./datasets/mnist_data/data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./datasets/mnist_data/data/MNIST/raw\n",
      "\n",
      "Datasets loaded. Train size: 60000, Test size: 10000\n",
      "Using device: cpu\n",
      "\n",
      "=== Training Standard PyTorch MLP ===\n",
      "Initializing network with standard PyTorch layers\n",
      "Model architecture:\n",
      "SimpleMLPModel(\n",
      "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n",
      "Epoch 1/10 started\n",
      "  Batch 100: Loss 2.1969, Accuracy 63.95%\n",
      "  Batch 200: Loss 1.5984, Accuracy 71.49%\n",
      "  Batch 300: Loss 1.3548, Accuracy 74.72%\n",
      "  Batch 400: Loss 1.2206, Accuracy 76.70%\n",
      "  Batch 500: Loss 1.1288, Accuracy 78.06%\n",
      "  Batch 600: Loss 1.0615, Accuracy 79.04%\n",
      "  Batch 700: Loss 1.0112, Accuracy 79.73%\n",
      "  Batch 800: Loss 0.9688, Accuracy 80.31%\n",
      "  Batch 900: Loss 0.9309, Accuracy 80.84%\n",
      "Epoch 1 completed in 12.10s. Training Loss: 0.9202, Accuracy: 81.01%\n",
      "Test Loss: 0.3527, Test Accuracy: 91.99%\n",
      "Saved new best model to mnist_mlp_standard_best.pt with accuracy: 91.99%\n",
      "Epoch 2/10 started\n",
      "  Batch 100: Loss 0.6149, Accuracy 85.64%\n",
      "  Batch 200: Loss 0.6202, Accuracy 85.61%\n",
      "  Batch 300: Loss 0.6197, Accuracy 85.34%\n",
      "  Batch 400: Loss 0.6089, Accuracy 85.70%\n",
      "  Batch 500: Loss 0.5942, Accuracy 86.04%\n",
      "  Batch 600: Loss 0.5910, Accuracy 86.07%\n",
      "  Batch 700: Loss 0.5839, Accuracy 86.22%\n",
      "  Batch 800: Loss 0.5791, Accuracy 86.35%\n",
      "  Batch 900: Loss 0.5781, Accuracy 86.43%\n",
      "Epoch 2 completed in 11.73s. Training Loss: 0.5770, Accuracy: 86.46%\n",
      "Test Loss: 0.3105, Test Accuracy: 92.45%\n",
      "Saved new best model to mnist_mlp_standard_best.pt with accuracy: 92.45%\n",
      "Epoch 3/10 started\n",
      "  Batch 100: Loss 0.5111, Accuracy 87.78%\n",
      "  Batch 200: Loss 0.5015, Accuracy 87.92%\n",
      "  Batch 300: Loss 0.4982, Accuracy 87.78%\n",
      "  Batch 400: Loss 0.4924, Accuracy 87.86%\n",
      "  Batch 500: Loss 0.4912, Accuracy 87.94%\n",
      "  Batch 600: Loss 0.4894, Accuracy 87.93%\n",
      "  Batch 700: Loss 0.4850, Accuracy 88.02%\n",
      "  Batch 800: Loss 0.4859, Accuracy 88.02%\n",
      "  Batch 900: Loss 0.4856, Accuracy 88.03%\n",
      "Epoch 3 completed in 11.76s. Training Loss: 0.4866, Accuracy: 88.02%\n",
      "Test Loss: 0.2667, Test Accuracy: 92.97%\n",
      "Saved new best model to mnist_mlp_standard_best.pt with accuracy: 92.97%\n",
      "Epoch 4/10 started\n",
      "  Batch 100: Loss 0.4261, Accuracy 89.12%\n",
      "  Batch 200: Loss 0.4267, Accuracy 88.93%\n",
      "  Batch 300: Loss 0.4346, Accuracy 88.86%\n",
      "  Batch 400: Loss 0.4290, Accuracy 88.95%\n",
      "  Batch 500: Loss 0.4282, Accuracy 88.86%\n",
      "  Batch 600: Loss 0.4262, Accuracy 88.99%\n",
      "  Batch 700: Loss 0.4253, Accuracy 89.01%\n",
      "  Batch 800: Loss 0.4209, Accuracy 89.12%\n",
      "  Batch 900: Loss 0.4184, Accuracy 89.13%\n",
      "Epoch 4 completed in 11.78s. Training Loss: 0.4170, Accuracy: 89.19%\n",
      "Test Loss: 0.2359, Test Accuracy: 93.92%\n",
      "Saved new best model to mnist_mlp_standard_best.pt with accuracy: 93.92%\n",
      "Epoch 5/10 started\n",
      "  Batch 100: Loss 0.3886, Accuracy 89.70%\n",
      "  Batch 200: Loss 0.3788, Accuracy 89.82%\n",
      "  Batch 300: Loss 0.3651, Accuracy 90.15%\n",
      "  Batch 400: Loss 0.3603, Accuracy 90.34%\n",
      "  Batch 500: Loss 0.3632, Accuracy 90.33%\n",
      "  Batch 600: Loss 0.3601, Accuracy 90.30%\n",
      "  Batch 700: Loss 0.3572, Accuracy 90.43%\n",
      "  Batch 800: Loss 0.3543, Accuracy 90.44%\n",
      "  Batch 900: Loss 0.3509, Accuracy 90.53%\n",
      "Epoch 5 completed in 11.80s. Training Loss: 0.3506, Accuracy: 90.51%\n",
      "Test Loss: 0.2036, Test Accuracy: 94.47%\n",
      "Saved new best model to mnist_mlp_standard_best.pt with accuracy: 94.47%\n",
      "Epoch 6/10 started\n",
      "  Batch 100: Loss 0.3053, Accuracy 91.80%\n",
      "  Batch 200: Loss 0.3087, Accuracy 91.66%\n",
      "  Batch 300: Loss 0.3072, Accuracy 91.73%\n",
      "  Batch 400: Loss 0.3047, Accuracy 91.75%\n",
      "  Batch 500: Loss 0.3027, Accuracy 91.74%\n",
      "  Batch 600: Loss 0.3058, Accuracy 91.72%\n",
      "  Batch 700: Loss 0.3065, Accuracy 91.67%\n",
      "  Batch 800: Loss 0.3091, Accuracy 91.59%\n",
      "  Batch 900: Loss 0.3053, Accuracy 91.65%\n",
      "Epoch 6 completed in 12.07s. Training Loss: 0.3049, Accuracy: 91.67%\n",
      "Test Loss: 0.1844, Test Accuracy: 95.02%\n",
      "Saved new best model to mnist_mlp_standard_best.pt with accuracy: 95.02%\n",
      "Epoch 7/10 started\n",
      "  Batch 100: Loss 0.2743, Accuracy 92.34%\n",
      "  Batch 200: Loss 0.2687, Accuracy 92.44%\n",
      "  Batch 300: Loss 0.2689, Accuracy 92.38%\n",
      "  Batch 400: Loss 0.2653, Accuracy 92.39%\n",
      "  Batch 500: Loss 0.2668, Accuracy 92.42%\n",
      "  Batch 600: Loss 0.2713, Accuracy 92.33%\n",
      "  Batch 700: Loss 0.2745, Accuracy 92.28%\n",
      "  Batch 800: Loss 0.2723, Accuracy 92.31%\n",
      "  Batch 900: Loss 0.2690, Accuracy 92.36%\n",
      "Epoch 7 completed in 11.44s. Training Loss: 0.2683, Accuracy: 92.39%\n",
      "Test Loss: 0.1708, Test Accuracy: 95.33%\n",
      "Saved new best model to mnist_mlp_standard_best.pt with accuracy: 95.33%\n",
      "Epoch 8/10 started\n",
      "  Batch 100: Loss 0.2397, Accuracy 93.05%\n",
      "  Batch 200: Loss 0.2421, Accuracy 93.10%\n",
      "  Batch 300: Loss 0.2417, Accuracy 93.18%\n",
      "  Batch 400: Loss 0.2425, Accuracy 93.08%\n",
      "  Batch 500: Loss 0.2412, Accuracy 93.09%\n",
      "  Batch 600: Loss 0.2413, Accuracy 93.02%\n",
      "  Batch 700: Loss 0.2435, Accuracy 93.02%\n",
      "  Batch 800: Loss 0.2406, Accuracy 93.08%\n",
      "  Batch 900: Loss 0.2377, Accuracy 93.16%\n",
      "Epoch 8 completed in 11.84s. Training Loss: 0.2369, Accuracy: 93.17%\n",
      "Test Loss: 0.1582, Test Accuracy: 95.66%\n",
      "Saved new best model to mnist_mlp_standard_best.pt with accuracy: 95.66%\n",
      "Epoch 9/10 started\n",
      "  Batch 100: Loss 0.2173, Accuracy 93.41%\n",
      "  Batch 200: Loss 0.2075, Accuracy 93.87%\n",
      "  Batch 300: Loss 0.2145, Accuracy 93.56%\n",
      "  Batch 400: Loss 0.2118, Accuracy 93.60%\n",
      "  Batch 500: Loss 0.2115, Accuracy 93.68%\n",
      "  Batch 600: Loss 0.2097, Accuracy 93.69%\n",
      "  Batch 700: Loss 0.2118, Accuracy 93.69%\n",
      "  Batch 800: Loss 0.2128, Accuracy 93.70%\n",
      "  Batch 900: Loss 0.2123, Accuracy 93.74%\n",
      "Epoch 9 completed in 11.40s. Training Loss: 0.2131, Accuracy: 93.72%\n",
      "Test Loss: 0.1476, Test Accuracy: 96.03%\n",
      "Saved new best model to mnist_mlp_standard_best.pt with accuracy: 96.03%\n",
      "Epoch 10/10 started\n",
      "  Batch 100: Loss 0.2085, Accuracy 93.73%\n",
      "  Batch 200: Loss 0.1911, Accuracy 94.09%\n",
      "  Batch 300: Loss 0.1918, Accuracy 94.07%\n",
      "  Batch 400: Loss 0.1940, Accuracy 94.02%\n",
      "  Batch 500: Loss 0.1940, Accuracy 94.09%\n",
      "  Batch 600: Loss 0.1946, Accuracy 94.10%\n",
      "  Batch 700: Loss 0.1951, Accuracy 94.14%\n",
      "  Batch 800: Loss 0.1929, Accuracy 94.21%\n",
      "  Batch 900: Loss 0.1914, Accuracy 94.23%\n",
      "Epoch 10 completed in 11.40s. Training Loss: 0.1921, Accuracy: 94.23%\n",
      "Test Loss: 0.1364, Test Accuracy: 96.36%\n",
      "Saved new best model to mnist_mlp_standard_best.pt with accuracy: 96.36%\n",
      "Standard PyTorch model achieved 96.36% accuracy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "torch.set_num_threads(2)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "train_dataset = MNIST('./datasets/mnist_data/data', train=True, download=True, transform=transform)\n",
    "test_dataset = MNIST('./datasets/mnist_data/data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False, num_workers=0)\n",
    "print(f\"Datasets loaded. Train size: {len(train_dataset)}, Test size: {len(test_dataset)}\")\n",
    "\n",
    "\n",
    "class SimpleMLPModel(nn.Module):\n",
    "    def __init__(self, use_adapt=False, axx_mult='mul8s_acc'):\n",
    "        super(SimpleMLPModel, self).__init__()\n",
    "\n",
    "        self.use_adapt = False  # Force disable AdaPT\n",
    "        self.axx_mult = None\n",
    "\n",
    "        print(\"Initializing network with standard PyTorch layers\")\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        gain = nn.init.calculate_gain('sigmoid')\n",
    "        nn.init.xavier_normal_(self.fc1.weight, gain=gain)\n",
    "        nn.init.zeros_(self.fc1.bias)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        nn.init.normal_(self.fc2.weight, mean=0, std=1)\n",
    "        nn.init.zeros_(self.fc2.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 4 or x.dim() == 3:\n",
    "            x = x.view(-1, 784)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "def train_model(model, device, train_loader, test_loader, epochs=5):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        start_time = time.time()\n",
    "        batch_count = 0\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} started\")\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            batch_count += 1\n",
    "\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "            if batch_idx % 100 == 99:\n",
    "                avg_loss = running_loss / batch_count\n",
    "                accuracy = 100. * correct / total\n",
    "                print(f'  Batch {batch_idx + 1}: Loss {avg_loss:.4f}, Accuracy {accuracy:.2f}%')\n",
    "\n",
    "        epoch_loss = running_loss / batch_count\n",
    "        epoch_acc = 100. * correct / total\n",
    "        epoch_time = time.time() - start_time\n",
    "        print(f\"Epoch {epoch + 1} completed in {epoch_time:.2f}s. Training Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%\")\n",
    "\n",
    "        test_loss, test_accuracy = evaluate_model(model, device, test_loader, criterion)\n",
    "        print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "        if test_accuracy > best_accuracy:\n",
    "            best_accuracy = test_accuracy\n",
    "            filename = 'mnist_mlp_standard_best.pt'\n",
    "            try:\n",
    "                torch.save(model.state_dict(), filename)\n",
    "                print(f\"Saved new best model to {filename} with accuracy: {best_accuracy:.2f}%\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not save model: {e}\")\n",
    "\n",
    "    return best_accuracy\n",
    "\n",
    "\n",
    "def evaluate_model(model, device, test_loader, criterion=None):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    if criterion is None:\n",
    "        criterion = nn.NLLLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "            total += target.size(0)\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "\n",
    "    return test_loss, accuracy\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    print(\"\\n=== Training Standard PyTorch MLP ===\")\n",
    "    model_standard = SimpleMLPModel(use_adapt=False)\n",
    "    print(f\"Model architecture:\\n{model_standard}\")\n",
    "\n",
    "    try:\n",
    "        accuracy_standard = train_model(model_standard, device, train_loader, test_loader, epochs=10)\n",
    "        print(f\"Standard PyTorch model achieved {accuracy_standard:.2f}% accuracy\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
