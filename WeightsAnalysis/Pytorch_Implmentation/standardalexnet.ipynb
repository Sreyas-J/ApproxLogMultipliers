{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-20T18:44:15.743229Z",
     "iopub.status.busy": "2025-04-20T18:44:15.743026Z",
     "iopub.status.idle": "2025-04-20T18:53:03.629255Z",
     "shell.execute_reply": "2025-04-20T18:53:03.628457Z",
     "shell.execute_reply.started": "2025-04-20T18:44:15.743210Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "CIFAR-10 dataset not found, downloading to working directory\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./output/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [00:04<00:00, 35.1MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./output/cifar-10-python.tar.gz to ./output\n",
      "Files already downloaded and verified\n",
      "Starting training for 60 epochs\n",
      "Epoch: 1 | Batch: 0/352 | Loss: 2.3201 | Acc: 8.59%\n",
      "Epoch: 1 | Batch: 100/352 | Loss: 2.1198 | Acc: 20.87%\n",
      "Epoch: 1 | Batch: 200/352 | Loss: 1.9748 | Acc: 26.32%\n",
      "Epoch: 1 | Batch: 300/352 | Loss: 1.8725 | Acc: 30.39%\n",
      "Epoch 1/60\n",
      "Train Loss: 1.8294, Train Acc: 31.92%\n",
      "Val Loss: 1.5636, Val Acc: 42.00%\n",
      "Saving best model with validation loss: 1.5636\n",
      "Epoch: 2 | Batch: 0/352 | Loss: 1.5084 | Acc: 46.09%\n",
      "Epoch: 2 | Batch: 100/352 | Loss: 1.5258 | Acc: 43.50%\n",
      "Epoch: 2 | Batch: 200/352 | Loss: 1.4827 | Acc: 45.16%\n",
      "Epoch: 2 | Batch: 300/352 | Loss: 1.4433 | Acc: 46.73%\n",
      "Epoch 2/60\n",
      "Train Loss: 1.4280, Train Acc: 47.50%\n",
      "Val Loss: 1.3389, Val Acc: 51.22%\n",
      "Saving best model with validation loss: 1.3389\n",
      "Epoch: 3 | Batch: 0/352 | Loss: 1.3097 | Acc: 46.09%\n",
      "Epoch: 3 | Batch: 100/352 | Loss: 1.2965 | Acc: 52.65%\n",
      "Epoch: 3 | Batch: 200/352 | Loss: 1.2574 | Acc: 54.40%\n",
      "Epoch: 3 | Batch: 300/352 | Loss: 1.2320 | Acc: 55.61%\n",
      "Epoch 3/60\n",
      "Train Loss: 1.2177, Train Acc: 56.17%\n",
      "Val Loss: 1.0831, Val Acc: 61.68%\n",
      "Saving best model with validation loss: 1.0831\n",
      "Epoch: 4 | Batch: 0/352 | Loss: 1.1460 | Acc: 58.59%\n",
      "Epoch: 4 | Batch: 100/352 | Loss: 1.1118 | Acc: 60.31%\n",
      "Epoch: 4 | Batch: 200/352 | Loss: 1.0963 | Acc: 60.70%\n",
      "Epoch: 4 | Batch: 300/352 | Loss: 1.0781 | Acc: 61.41%\n",
      "Epoch 4/60\n",
      "Train Loss: 1.0735, Train Acc: 61.63%\n",
      "Val Loss: 0.9650, Val Acc: 65.06%\n",
      "Saving best model with validation loss: 0.9650\n",
      "Epoch: 5 | Batch: 0/352 | Loss: 0.9372 | Acc: 67.19%\n",
      "Epoch: 5 | Batch: 100/352 | Loss: 0.9794 | Acc: 65.22%\n",
      "Epoch: 5 | Batch: 200/352 | Loss: 0.9741 | Acc: 65.52%\n",
      "Epoch: 5 | Batch: 300/352 | Loss: 0.9710 | Acc: 65.56%\n",
      "Epoch 5/60\n",
      "Train Loss: 0.9617, Train Acc: 65.91%\n",
      "Val Loss: 0.8749, Val Acc: 69.02%\n",
      "Saving best model with validation loss: 0.8749\n",
      "Epoch: 6 | Batch: 0/352 | Loss: 0.9978 | Acc: 65.62%\n",
      "Epoch: 6 | Batch: 100/352 | Loss: 0.8792 | Acc: 69.30%\n",
      "Epoch: 6 | Batch: 200/352 | Loss: 0.8695 | Acc: 69.33%\n",
      "Epoch: 6 | Batch: 300/352 | Loss: 0.8764 | Acc: 69.10%\n",
      "Epoch 6/60\n",
      "Train Loss: 0.8747, Train Acc: 69.21%\n",
      "Val Loss: 0.8009, Val Acc: 71.38%\n",
      "Saving best model with validation loss: 0.8009\n",
      "Epoch: 7 | Batch: 0/352 | Loss: 0.9620 | Acc: 62.50%\n",
      "Epoch: 7 | Batch: 100/352 | Loss: 0.8267 | Acc: 71.06%\n",
      "Epoch: 7 | Batch: 200/352 | Loss: 0.8384 | Acc: 70.58%\n",
      "Epoch: 7 | Batch: 300/352 | Loss: 0.8283 | Acc: 70.84%\n",
      "Epoch 7/60\n",
      "Train Loss: 0.8237, Train Acc: 70.97%\n",
      "Val Loss: 0.7686, Val Acc: 72.22%\n",
      "Saving best model with validation loss: 0.7686\n",
      "Epoch: 8 | Batch: 0/352 | Loss: 0.8069 | Acc: 67.97%\n",
      "Epoch: 8 | Batch: 100/352 | Loss: 0.7735 | Acc: 72.59%\n",
      "Epoch: 8 | Batch: 200/352 | Loss: 0.7641 | Acc: 72.91%\n",
      "Epoch: 8 | Batch: 300/352 | Loss: 0.7694 | Acc: 72.83%\n",
      "Epoch 8/60\n",
      "Train Loss: 0.7659, Train Acc: 73.02%\n",
      "Val Loss: 0.7004, Val Acc: 75.88%\n",
      "Saving best model with validation loss: 0.7004\n",
      "Epoch: 9 | Batch: 0/352 | Loss: 0.6817 | Acc: 74.22%\n",
      "Epoch: 9 | Batch: 100/352 | Loss: 0.7309 | Acc: 74.89%\n",
      "Epoch: 9 | Batch: 200/352 | Loss: 0.7346 | Acc: 74.42%\n",
      "Epoch: 9 | Batch: 300/352 | Loss: 0.7253 | Acc: 74.70%\n",
      "Epoch 9/60\n",
      "Train Loss: 0.7213, Train Acc: 74.84%\n",
      "Val Loss: 0.6533, Val Acc: 76.58%\n",
      "Saving best model with validation loss: 0.6533\n",
      "Epoch: 10 | Batch: 0/352 | Loss: 0.7634 | Acc: 72.66%\n",
      "Epoch: 10 | Batch: 100/352 | Loss: 0.6958 | Acc: 75.77%\n",
      "Epoch: 10 | Batch: 200/352 | Loss: 0.6858 | Acc: 76.17%\n",
      "Epoch: 10 | Batch: 300/352 | Loss: 0.6815 | Acc: 76.36%\n",
      "Epoch 10/60\n",
      "Train Loss: 0.6814, Train Acc: 76.40%\n",
      "Val Loss: 0.6542, Val Acc: 76.44%\n",
      "Epoch: 11 | Batch: 0/352 | Loss: 0.7694 | Acc: 73.44%\n",
      "Epoch: 11 | Batch: 100/352 | Loss: 0.6387 | Acc: 77.55%\n",
      "Epoch: 11 | Batch: 200/352 | Loss: 0.6461 | Acc: 77.29%\n",
      "Epoch: 11 | Batch: 300/352 | Loss: 0.6481 | Acc: 77.37%\n",
      "Epoch 11/60\n",
      "Train Loss: 0.6453, Train Acc: 77.55%\n",
      "Val Loss: 0.6377, Val Acc: 77.42%\n",
      "Saving best model with validation loss: 0.6377\n",
      "Epoch: 12 | Batch: 0/352 | Loss: 0.7045 | Acc: 75.78%\n",
      "Epoch: 12 | Batch: 100/352 | Loss: 0.6172 | Acc: 79.01%\n",
      "Epoch: 12 | Batch: 200/352 | Loss: 0.6119 | Acc: 78.88%\n",
      "Epoch: 12 | Batch: 300/352 | Loss: 0.6125 | Acc: 78.79%\n",
      "Epoch 12/60\n",
      "Train Loss: 0.6136, Train Acc: 78.71%\n",
      "Val Loss: 0.6343, Val Acc: 77.06%\n",
      "Saving best model with validation loss: 0.6343\n",
      "Epoch: 13 | Batch: 0/352 | Loss: 0.5997 | Acc: 80.47%\n",
      "Epoch: 13 | Batch: 100/352 | Loss: 0.5970 | Acc: 78.91%\n",
      "Epoch: 13 | Batch: 200/352 | Loss: 0.5901 | Acc: 79.32%\n",
      "Epoch: 13 | Batch: 300/352 | Loss: 0.5893 | Acc: 79.37%\n",
      "Epoch 13/60\n",
      "Train Loss: 0.5910, Train Acc: 79.35%\n",
      "Val Loss: 0.5838, Val Acc: 78.92%\n",
      "Saving best model with validation loss: 0.5838\n",
      "Epoch: 14 | Batch: 0/352 | Loss: 0.6888 | Acc: 72.66%\n",
      "Epoch: 14 | Batch: 100/352 | Loss: 0.5533 | Acc: 80.69%\n",
      "Epoch: 14 | Batch: 200/352 | Loss: 0.5595 | Acc: 80.40%\n",
      "Epoch: 14 | Batch: 300/352 | Loss: 0.5623 | Acc: 80.37%\n",
      "Epoch 14/60\n",
      "Train Loss: 0.5615, Train Acc: 80.42%\n",
      "Val Loss: 0.5672, Val Acc: 80.14%\n",
      "Saving best model with validation loss: 0.5672\n",
      "Epoch: 15 | Batch: 0/352 | Loss: 0.5000 | Acc: 81.25%\n",
      "Epoch: 15 | Batch: 100/352 | Loss: 0.5478 | Acc: 81.37%\n",
      "Epoch: 15 | Batch: 200/352 | Loss: 0.5437 | Acc: 81.20%\n",
      "Epoch: 15 | Batch: 300/352 | Loss: 0.5434 | Acc: 81.12%\n",
      "Epoch 15/60\n",
      "Train Loss: 0.5424, Train Acc: 81.17%\n",
      "Val Loss: 0.5923, Val Acc: 79.00%\n",
      "Epoch: 16 | Batch: 0/352 | Loss: 0.6067 | Acc: 82.03%\n",
      "Epoch: 16 | Batch: 100/352 | Loss: 0.5057 | Acc: 82.53%\n",
      "Epoch: 16 | Batch: 200/352 | Loss: 0.5093 | Acc: 82.48%\n",
      "Epoch: 16 | Batch: 300/352 | Loss: 0.5137 | Acc: 82.20%\n",
      "Epoch 16/60\n",
      "Train Loss: 0.5168, Train Acc: 82.16%\n",
      "Val Loss: 0.5305, Val Acc: 81.32%\n",
      "Saving best model with validation loss: 0.5305\n",
      "Epoch: 17 | Batch: 0/352 | Loss: 0.4815 | Acc: 83.59%\n",
      "Epoch: 17 | Batch: 100/352 | Loss: 0.4995 | Acc: 83.01%\n",
      "Epoch: 17 | Batch: 200/352 | Loss: 0.4955 | Acc: 82.88%\n",
      "Epoch: 17 | Batch: 300/352 | Loss: 0.5038 | Acc: 82.55%\n",
      "Epoch 17/60\n",
      "Train Loss: 0.5016, Train Acc: 82.64%\n",
      "Val Loss: 0.5348, Val Acc: 80.62%\n",
      "Epoch: 18 | Batch: 0/352 | Loss: 0.4215 | Acc: 86.72%\n",
      "Epoch: 18 | Batch: 100/352 | Loss: 0.4637 | Acc: 83.71%\n",
      "Epoch: 18 | Batch: 200/352 | Loss: 0.4738 | Acc: 83.25%\n",
      "Epoch: 18 | Batch: 300/352 | Loss: 0.4741 | Acc: 83.33%\n",
      "Epoch 18/60\n",
      "Train Loss: 0.4766, Train Acc: 83.21%\n",
      "Val Loss: 0.5145, Val Acc: 82.26%\n",
      "Saving best model with validation loss: 0.5145\n",
      "Epoch: 19 | Batch: 0/352 | Loss: 0.4177 | Acc: 85.16%\n",
      "Epoch: 19 | Batch: 100/352 | Loss: 0.4560 | Acc: 84.36%\n",
      "Epoch: 19 | Batch: 200/352 | Loss: 0.4581 | Acc: 84.07%\n",
      "Epoch: 19 | Batch: 300/352 | Loss: 0.4606 | Acc: 83.90%\n",
      "Epoch 19/60\n",
      "Train Loss: 0.4603, Train Acc: 83.85%\n",
      "Val Loss: 0.5017, Val Acc: 81.94%\n",
      "Saving best model with validation loss: 0.5017\n",
      "Epoch: 20 | Batch: 0/352 | Loss: 0.4810 | Acc: 84.38%\n",
      "Epoch: 20 | Batch: 100/352 | Loss: 0.4383 | Acc: 84.61%\n",
      "Epoch: 20 | Batch: 200/352 | Loss: 0.4454 | Acc: 84.48%\n",
      "Epoch: 20 | Batch: 300/352 | Loss: 0.4481 | Acc: 84.39%\n",
      "Epoch 20/60\n",
      "Train Loss: 0.4468, Train Acc: 84.45%\n",
      "Val Loss: 0.4898, Val Acc: 82.56%\n",
      "Saving best model with validation loss: 0.4898\n",
      "Epoch: 21 | Batch: 0/352 | Loss: 0.4052 | Acc: 86.72%\n",
      "Epoch: 21 | Batch: 100/352 | Loss: 0.4195 | Acc: 85.30%\n",
      "Epoch: 21 | Batch: 200/352 | Loss: 0.4253 | Acc: 85.28%\n",
      "Epoch: 21 | Batch: 300/352 | Loss: 0.4312 | Acc: 85.14%\n",
      "Epoch 21/60\n",
      "Train Loss: 0.4302, Train Acc: 85.20%\n",
      "Val Loss: 0.4722, Val Acc: 83.22%\n",
      "Saving best model with validation loss: 0.4722\n",
      "Epoch: 22 | Batch: 0/352 | Loss: 0.4076 | Acc: 84.38%\n",
      "Epoch: 22 | Batch: 100/352 | Loss: 0.4014 | Acc: 85.88%\n",
      "Epoch: 22 | Batch: 200/352 | Loss: 0.4089 | Acc: 85.74%\n",
      "Epoch: 22 | Batch: 300/352 | Loss: 0.4059 | Acc: 85.83%\n",
      "Epoch 22/60\n",
      "Train Loss: 0.4076, Train Acc: 85.79%\n",
      "Val Loss: 0.5102, Val Acc: 81.62%\n",
      "Epoch: 23 | Batch: 0/352 | Loss: 0.4394 | Acc: 85.16%\n",
      "Epoch: 23 | Batch: 100/352 | Loss: 0.3907 | Acc: 86.38%\n",
      "Epoch: 23 | Batch: 200/352 | Loss: 0.3966 | Acc: 86.23%\n",
      "Epoch: 23 | Batch: 300/352 | Loss: 0.3960 | Acc: 86.28%\n",
      "Epoch 23/60\n",
      "Train Loss: 0.3943, Train Acc: 86.36%\n",
      "Val Loss: 0.4626, Val Acc: 83.40%\n",
      "Saving best model with validation loss: 0.4626\n",
      "Epoch: 24 | Batch: 0/352 | Loss: 0.4306 | Acc: 82.81%\n",
      "Epoch: 24 | Batch: 100/352 | Loss: 0.3723 | Acc: 87.31%\n",
      "Epoch: 24 | Batch: 200/352 | Loss: 0.3811 | Acc: 86.89%\n",
      "Epoch: 24 | Batch: 300/352 | Loss: 0.3843 | Acc: 86.77%\n",
      "Epoch 24/60\n",
      "Train Loss: 0.3863, Train Acc: 86.77%\n",
      "Val Loss: 0.4839, Val Acc: 82.92%\n",
      "Epoch: 25 | Batch: 0/352 | Loss: 0.2458 | Acc: 91.41%\n",
      "Epoch: 25 | Batch: 100/352 | Loss: 0.3695 | Acc: 87.26%\n",
      "Epoch: 25 | Batch: 200/352 | Loss: 0.3737 | Acc: 87.10%\n",
      "Epoch: 25 | Batch: 300/352 | Loss: 0.3702 | Acc: 87.18%\n",
      "Epoch 25/60\n",
      "Train Loss: 0.3686, Train Acc: 87.27%\n",
      "Val Loss: 0.4412, Val Acc: 84.54%\n",
      "Saving best model with validation loss: 0.4412\n",
      "Epoch: 26 | Batch: 0/352 | Loss: 0.2702 | Acc: 91.41%\n",
      "Epoch: 26 | Batch: 100/352 | Loss: 0.3390 | Acc: 88.24%\n",
      "Epoch: 26 | Batch: 200/352 | Loss: 0.3420 | Acc: 88.12%\n",
      "Epoch: 26 | Batch: 300/352 | Loss: 0.3499 | Acc: 87.79%\n",
      "Epoch 26/60\n",
      "Train Loss: 0.3527, Train Acc: 87.76%\n",
      "Val Loss: 0.4504, Val Acc: 84.82%\n",
      "Epoch: 27 | Batch: 0/352 | Loss: 0.3390 | Acc: 87.50%\n",
      "Epoch: 27 | Batch: 100/352 | Loss: 0.3349 | Acc: 88.30%\n",
      "Epoch: 27 | Batch: 200/352 | Loss: 0.3453 | Acc: 88.04%\n",
      "Epoch: 27 | Batch: 300/352 | Loss: 0.3483 | Acc: 87.92%\n",
      "Epoch 27/60\n",
      "Train Loss: 0.3489, Train Acc: 87.95%\n",
      "Val Loss: 0.4629, Val Acc: 83.98%\n",
      "Epoch: 28 | Batch: 0/352 | Loss: 0.2603 | Acc: 93.75%\n",
      "Epoch: 28 | Batch: 100/352 | Loss: 0.3252 | Acc: 88.89%\n",
      "Epoch: 28 | Batch: 200/352 | Loss: 0.3296 | Acc: 88.69%\n",
      "Epoch: 28 | Batch: 300/352 | Loss: 0.3316 | Acc: 88.63%\n",
      "Epoch 28/60\n",
      "Train Loss: 0.3331, Train Acc: 88.49%\n",
      "Val Loss: 0.4423, Val Acc: 84.38%\n",
      "Epoch: 29 | Batch: 0/352 | Loss: 0.3838 | Acc: 83.59%\n",
      "Epoch: 29 | Batch: 100/352 | Loss: 0.3193 | Acc: 88.70%\n",
      "Epoch: 29 | Batch: 200/352 | Loss: 0.3234 | Acc: 88.65%\n",
      "Epoch: 29 | Batch: 300/352 | Loss: 0.3252 | Acc: 88.58%\n",
      "Epoch 29/60\n",
      "Train Loss: 0.3262, Train Acc: 88.59%\n",
      "Val Loss: 0.4292, Val Acc: 84.78%\n",
      "Saving best model with validation loss: 0.4292\n",
      "Epoch: 30 | Batch: 0/352 | Loss: 0.2860 | Acc: 89.84%\n",
      "Epoch: 30 | Batch: 100/352 | Loss: 0.3075 | Acc: 89.16%\n",
      "Epoch: 30 | Batch: 200/352 | Loss: 0.3088 | Acc: 89.12%\n",
      "Epoch: 30 | Batch: 300/352 | Loss: 0.3114 | Acc: 89.15%\n",
      "Epoch 30/60\n",
      "Train Loss: 0.3162, Train Acc: 89.00%\n",
      "Val Loss: 0.4141, Val Acc: 85.48%\n",
      "Saving best model with validation loss: 0.4141\n",
      "Epoch: 31 | Batch: 0/352 | Loss: 0.1965 | Acc: 92.97%\n",
      "Epoch: 31 | Batch: 100/352 | Loss: 0.2473 | Acc: 91.83%\n",
      "Epoch: 31 | Batch: 200/352 | Loss: 0.2407 | Acc: 91.83%\n",
      "Epoch: 31 | Batch: 300/352 | Loss: 0.2366 | Acc: 91.93%\n",
      "Epoch 31/60\n",
      "Train Loss: 0.2345, Train Acc: 92.01%\n",
      "Val Loss: 0.3722, Val Acc: 86.80%\n",
      "Saving best model with validation loss: 0.3722\n",
      "Epoch: 32 | Batch: 0/352 | Loss: 0.2966 | Acc: 88.28%\n",
      "Epoch: 32 | Batch: 100/352 | Loss: 0.2155 | Acc: 92.64%\n",
      "Epoch: 32 | Batch: 200/352 | Loss: 0.2146 | Acc: 92.64%\n",
      "Epoch: 32 | Batch: 300/352 | Loss: 0.2137 | Acc: 92.63%\n",
      "Epoch 32/60\n",
      "Train Loss: 0.2135, Train Acc: 92.63%\n",
      "Val Loss: 0.3714, Val Acc: 87.18%\n",
      "Saving best model with validation loss: 0.3714\n",
      "Epoch: 33 | Batch: 0/352 | Loss: 0.2242 | Acc: 91.41%\n",
      "Epoch: 33 | Batch: 100/352 | Loss: 0.2034 | Acc: 93.13%\n",
      "Epoch: 33 | Batch: 200/352 | Loss: 0.2003 | Acc: 93.23%\n",
      "Epoch: 33 | Batch: 300/352 | Loss: 0.2023 | Acc: 93.16%\n",
      "Epoch 33/60\n",
      "Train Loss: 0.2031, Train Acc: 93.07%\n",
      "Val Loss: 0.3721, Val Acc: 87.28%\n",
      "Epoch: 34 | Batch: 0/352 | Loss: 0.1159 | Acc: 96.09%\n",
      "Epoch: 34 | Batch: 100/352 | Loss: 0.1917 | Acc: 93.44%\n",
      "Epoch: 34 | Batch: 200/352 | Loss: 0.1949 | Acc: 93.34%\n",
      "Epoch: 34 | Batch: 300/352 | Loss: 0.1962 | Acc: 93.24%\n",
      "Epoch 34/60\n",
      "Train Loss: 0.1961, Train Acc: 93.25%\n",
      "Val Loss: 0.3668, Val Acc: 87.28%\n",
      "Saving best model with validation loss: 0.3668\n",
      "Epoch: 35 | Batch: 0/352 | Loss: 0.1914 | Acc: 92.97%\n",
      "Epoch: 35 | Batch: 100/352 | Loss: 0.1936 | Acc: 93.17%\n",
      "Epoch: 35 | Batch: 200/352 | Loss: 0.1948 | Acc: 93.11%\n",
      "Epoch: 35 | Batch: 300/352 | Loss: 0.1925 | Acc: 93.32%\n",
      "Epoch 35/60\n",
      "Train Loss: 0.1919, Train Acc: 93.31%\n",
      "Val Loss: 0.3657, Val Acc: 87.34%\n",
      "Saving best model with validation loss: 0.3657\n",
      "Epoch: 36 | Batch: 0/352 | Loss: 0.1498 | Acc: 95.31%\n",
      "Epoch: 36 | Batch: 100/352 | Loss: 0.1921 | Acc: 93.22%\n",
      "Epoch: 36 | Batch: 200/352 | Loss: 0.1876 | Acc: 93.53%\n",
      "Epoch: 36 | Batch: 300/352 | Loss: 0.1857 | Acc: 93.64%\n",
      "Epoch 36/60\n",
      "Train Loss: 0.1858, Train Acc: 93.62%\n",
      "Val Loss: 0.3542, Val Acc: 88.12%\n",
      "Saving best model with validation loss: 0.3542\n",
      "Epoch: 37 | Batch: 0/352 | Loss: 0.1528 | Acc: 95.31%\n",
      "Epoch: 37 | Batch: 100/352 | Loss: 0.1880 | Acc: 93.57%\n",
      "Epoch: 37 | Batch: 200/352 | Loss: 0.1822 | Acc: 93.76%\n",
      "Epoch: 37 | Batch: 300/352 | Loss: 0.1824 | Acc: 93.74%\n",
      "Epoch 37/60\n",
      "Train Loss: 0.1848, Train Acc: 93.69%\n",
      "Val Loss: 0.3815, Val Acc: 87.16%\n",
      "Epoch: 38 | Batch: 0/352 | Loss: 0.1440 | Acc: 94.53%\n",
      "Epoch: 38 | Batch: 100/352 | Loss: 0.1761 | Acc: 93.94%\n",
      "Epoch: 38 | Batch: 200/352 | Loss: 0.1784 | Acc: 93.79%\n",
      "Epoch: 38 | Batch: 300/352 | Loss: 0.1786 | Acc: 93.80%\n",
      "Epoch 38/60\n",
      "Train Loss: 0.1792, Train Acc: 93.83%\n",
      "Val Loss: 0.3796, Val Acc: 87.44%\n",
      "Epoch: 39 | Batch: 0/352 | Loss: 0.1422 | Acc: 96.09%\n",
      "Epoch: 39 | Batch: 100/352 | Loss: 0.1736 | Acc: 93.91%\n",
      "Epoch: 39 | Batch: 200/352 | Loss: 0.1754 | Acc: 94.01%\n",
      "Epoch: 39 | Batch: 300/352 | Loss: 0.1779 | Acc: 93.87%\n",
      "Epoch 39/60\n",
      "Train Loss: 0.1777, Train Acc: 93.88%\n",
      "Val Loss: 0.3619, Val Acc: 87.92%\n",
      "Epoch: 40 | Batch: 0/352 | Loss: 0.1728 | Acc: 92.97%\n",
      "Epoch: 40 | Batch: 100/352 | Loss: 0.1735 | Acc: 94.14%\n",
      "Epoch: 40 | Batch: 200/352 | Loss: 0.1701 | Acc: 94.24%\n",
      "Epoch: 40 | Batch: 300/352 | Loss: 0.1706 | Acc: 94.11%\n",
      "Epoch 40/60\n",
      "Train Loss: 0.1706, Train Acc: 94.06%\n",
      "Val Loss: 0.3723, Val Acc: 87.40%\n",
      "Epoch: 41 | Batch: 0/352 | Loss: 0.1923 | Acc: 93.75%\n",
      "Epoch: 41 | Batch: 100/352 | Loss: 0.1739 | Acc: 94.06%\n",
      "Epoch: 41 | Batch: 200/352 | Loss: 0.1691 | Acc: 94.19%\n",
      "Epoch: 41 | Batch: 300/352 | Loss: 0.1686 | Acc: 94.17%\n",
      "Epoch 41/60\n",
      "Train Loss: 0.1693, Train Acc: 94.16%\n",
      "Val Loss: 0.3721, Val Acc: 87.46%\n",
      "Epoch: 42 | Batch: 0/352 | Loss: 0.1902 | Acc: 94.53%\n",
      "Epoch: 42 | Batch: 100/352 | Loss: 0.1689 | Acc: 94.34%\n",
      "Epoch: 42 | Batch: 200/352 | Loss: 0.1683 | Acc: 94.15%\n",
      "Epoch: 42 | Batch: 300/352 | Loss: 0.1695 | Acc: 94.15%\n",
      "Epoch 42/60\n",
      "Train Loss: 0.1690, Train Acc: 94.21%\n",
      "Val Loss: 0.3681, Val Acc: 87.64%\n",
      "Epoch: 43 | Batch: 0/352 | Loss: 0.1490 | Acc: 95.31%\n",
      "Epoch: 43 | Batch: 100/352 | Loss: 0.1614 | Acc: 94.69%\n",
      "Epoch: 43 | Batch: 200/352 | Loss: 0.1635 | Acc: 94.50%\n",
      "Epoch: 43 | Batch: 300/352 | Loss: 0.1654 | Acc: 94.29%\n",
      "Epoch 43/60\n",
      "Train Loss: 0.1655, Train Acc: 94.30%\n",
      "Val Loss: 0.3766, Val Acc: 87.74%\n",
      "Epoch: 44 | Batch: 0/352 | Loss: 0.1333 | Acc: 96.09%\n",
      "Epoch: 44 | Batch: 100/352 | Loss: 0.1697 | Acc: 94.19%\n",
      "Epoch: 44 | Batch: 200/352 | Loss: 0.1684 | Acc: 94.20%\n",
      "Epoch: 44 | Batch: 300/352 | Loss: 0.1653 | Acc: 94.33%\n",
      "Epoch 44/60\n",
      "Train Loss: 0.1634, Train Acc: 94.38%\n",
      "Val Loss: 0.3742, Val Acc: 87.84%\n",
      "Epoch: 45 | Batch: 0/352 | Loss: 0.1938 | Acc: 91.41%\n",
      "Epoch: 45 | Batch: 100/352 | Loss: 0.1536 | Acc: 94.79%\n",
      "Epoch: 45 | Batch: 200/352 | Loss: 0.1596 | Acc: 94.50%\n",
      "Epoch: 45 | Batch: 300/352 | Loss: 0.1604 | Acc: 94.51%\n",
      "Epoch 45/60\n",
      "Train Loss: 0.1607, Train Acc: 94.47%\n",
      "Val Loss: 0.3686, Val Acc: 87.64%\n",
      "Epoch: 46 | Batch: 0/352 | Loss: 0.1156 | Acc: 93.75%\n",
      "Epoch: 46 | Batch: 100/352 | Loss: 0.1534 | Acc: 94.79%\n",
      "Epoch: 46 | Batch: 200/352 | Loss: 0.1541 | Acc: 94.76%\n",
      "Epoch: 46 | Batch: 300/352 | Loss: 0.1564 | Acc: 94.63%\n",
      "Epoch 46/60\n",
      "Train Loss: 0.1567, Train Acc: 94.63%\n",
      "Val Loss: 0.3651, Val Acc: 87.74%\n",
      "Epoch: 47 | Batch: 0/352 | Loss: 0.1938 | Acc: 90.62%\n",
      "Epoch: 47 | Batch: 100/352 | Loss: 0.1546 | Acc: 94.64%\n",
      "Epoch: 47 | Batch: 200/352 | Loss: 0.1605 | Acc: 94.48%\n",
      "Epoch: 47 | Batch: 300/352 | Loss: 0.1589 | Acc: 94.51%\n",
      "Epoch 47/60\n",
      "Train Loss: 0.1598, Train Acc: 94.48%\n",
      "Val Loss: 0.3705, Val Acc: 87.50%\n",
      "Epoch: 48 | Batch: 0/352 | Loss: 0.1133 | Acc: 96.88%\n",
      "Epoch: 48 | Batch: 100/352 | Loss: 0.1524 | Acc: 94.77%\n",
      "Epoch: 48 | Batch: 200/352 | Loss: 0.1539 | Acc: 94.62%\n",
      "Epoch: 48 | Batch: 300/352 | Loss: 0.1529 | Acc: 94.69%\n",
      "Epoch 48/60\n",
      "Train Loss: 0.1514, Train Acc: 94.74%\n",
      "Val Loss: 0.3721, Val Acc: 87.52%\n",
      "Epoch: 49 | Batch: 0/352 | Loss: 0.1580 | Acc: 96.09%\n",
      "Epoch: 49 | Batch: 100/352 | Loss: 0.1486 | Acc: 94.89%\n",
      "Epoch: 49 | Batch: 200/352 | Loss: 0.1523 | Acc: 94.76%\n",
      "Epoch: 49 | Batch: 300/352 | Loss: 0.1512 | Acc: 94.78%\n",
      "Epoch 49/60\n",
      "Train Loss: 0.1516, Train Acc: 94.74%\n",
      "Val Loss: 0.3813, Val Acc: 87.60%\n",
      "Epoch: 50 | Batch: 0/352 | Loss: 0.1122 | Acc: 96.88%\n",
      "Epoch: 50 | Batch: 100/352 | Loss: 0.1433 | Acc: 95.07%\n",
      "Epoch: 50 | Batch: 200/352 | Loss: 0.1477 | Acc: 94.95%\n",
      "Epoch: 50 | Batch: 300/352 | Loss: 0.1472 | Acc: 94.99%\n",
      "Epoch 50/60\n",
      "Train Loss: 0.1481, Train Acc: 94.96%\n",
      "Val Loss: 0.3660, Val Acc: 88.00%\n",
      "Epoch: 51 | Batch: 0/352 | Loss: 0.0976 | Acc: 96.09%\n",
      "Epoch: 51 | Batch: 100/352 | Loss: 0.1437 | Acc: 95.06%\n",
      "Epoch: 51 | Batch: 200/352 | Loss: 0.1439 | Acc: 95.08%\n",
      "Epoch: 51 | Batch: 300/352 | Loss: 0.1434 | Acc: 95.07%\n",
      "Epoch 51/60\n",
      "Train Loss: 0.1445, Train Acc: 95.02%\n",
      "Val Loss: 0.3727, Val Acc: 87.90%\n",
      "Epoch: 52 | Batch: 0/352 | Loss: 0.2336 | Acc: 95.31%\n",
      "Epoch: 52 | Batch: 100/352 | Loss: 0.1462 | Acc: 95.05%\n",
      "Epoch: 52 | Batch: 200/352 | Loss: 0.1453 | Acc: 94.92%\n",
      "Epoch: 52 | Batch: 300/352 | Loss: 0.1470 | Acc: 94.91%\n",
      "Epoch 52/60\n",
      "Train Loss: 0.1468, Train Acc: 94.92%\n",
      "Val Loss: 0.3611, Val Acc: 87.88%\n",
      "Epoch: 53 | Batch: 0/352 | Loss: 0.2047 | Acc: 91.41%\n",
      "Epoch: 53 | Batch: 100/352 | Loss: 0.1432 | Acc: 94.98%\n",
      "Epoch: 53 | Batch: 200/352 | Loss: 0.1441 | Acc: 95.05%\n",
      "Epoch: 53 | Batch: 300/352 | Loss: 0.1433 | Acc: 95.09%\n",
      "Epoch 53/60\n",
      "Train Loss: 0.1434, Train Acc: 95.13%\n",
      "Val Loss: 0.3698, Val Acc: 87.92%\n",
      "Epoch: 54 | Batch: 0/352 | Loss: 0.1322 | Acc: 95.31%\n",
      "Epoch: 54 | Batch: 100/352 | Loss: 0.1319 | Acc: 95.57%\n",
      "Epoch: 54 | Batch: 200/352 | Loss: 0.1359 | Acc: 95.39%\n",
      "Epoch: 54 | Batch: 300/352 | Loss: 0.1357 | Acc: 95.35%\n",
      "Epoch 54/60\n",
      "Train Loss: 0.1365, Train Acc: 95.30%\n",
      "Val Loss: 0.3858, Val Acc: 87.32%\n",
      "Epoch: 55 | Batch: 0/352 | Loss: 0.1796 | Acc: 94.53%\n",
      "Epoch: 55 | Batch: 100/352 | Loss: 0.1364 | Acc: 95.17%\n",
      "Epoch: 55 | Batch: 200/352 | Loss: 0.1358 | Acc: 95.19%\n",
      "Epoch: 55 | Batch: 300/352 | Loss: 0.1356 | Acc: 95.25%\n",
      "Epoch 55/60\n",
      "Train Loss: 0.1384, Train Acc: 95.16%\n",
      "Val Loss: 0.3786, Val Acc: 87.92%\n",
      "Epoch: 56 | Batch: 0/352 | Loss: 0.1550 | Acc: 91.41%\n",
      "Epoch: 56 | Batch: 100/352 | Loss: 0.1402 | Acc: 94.93%\n",
      "Epoch: 56 | Batch: 200/352 | Loss: 0.1355 | Acc: 95.33%\n",
      "Epoch: 56 | Batch: 300/352 | Loss: 0.1361 | Acc: 95.32%\n",
      "Epoch 56/60\n",
      "Train Loss: 0.1364, Train Acc: 95.31%\n",
      "Val Loss: 0.3687, Val Acc: 87.96%\n",
      "Epoch: 57 | Batch: 0/352 | Loss: 0.1714 | Acc: 92.97%\n",
      "Epoch: 57 | Batch: 100/352 | Loss: 0.1301 | Acc: 95.45%\n",
      "Epoch: 57 | Batch: 200/352 | Loss: 0.1350 | Acc: 95.34%\n",
      "Epoch: 57 | Batch: 300/352 | Loss: 0.1351 | Acc: 95.32%\n",
      "Epoch 57/60\n",
      "Train Loss: 0.1346, Train Acc: 95.32%\n",
      "Val Loss: 0.3857, Val Acc: 87.42%\n",
      "Epoch: 58 | Batch: 0/352 | Loss: 0.0891 | Acc: 96.88%\n",
      "Epoch: 58 | Batch: 100/352 | Loss: 0.1304 | Acc: 95.42%\n",
      "Epoch: 58 | Batch: 200/352 | Loss: 0.1359 | Acc: 95.34%\n",
      "Epoch: 58 | Batch: 300/352 | Loss: 0.1350 | Acc: 95.36%\n",
      "Epoch 58/60\n",
      "Train Loss: 0.1355, Train Acc: 95.34%\n",
      "Val Loss: 0.3706, Val Acc: 87.94%\n",
      "Epoch: 59 | Batch: 0/352 | Loss: 0.0810 | Acc: 96.88%\n",
      "Epoch: 59 | Batch: 100/352 | Loss: 0.1312 | Acc: 95.44%\n",
      "Epoch: 59 | Batch: 200/352 | Loss: 0.1323 | Acc: 95.41%\n",
      "Epoch: 59 | Batch: 300/352 | Loss: 0.1308 | Acc: 95.45%\n",
      "Epoch 59/60\n",
      "Train Loss: 0.1314, Train Acc: 95.44%\n",
      "Val Loss: 0.3817, Val Acc: 87.98%\n",
      "Epoch: 60 | Batch: 0/352 | Loss: 0.1940 | Acc: 91.41%\n",
      "Epoch: 60 | Batch: 100/352 | Loss: 0.1314 | Acc: 95.66%\n",
      "Epoch: 60 | Batch: 200/352 | Loss: 0.1294 | Acc: 95.51%\n",
      "Epoch: 60 | Batch: 300/352 | Loss: 0.1309 | Acc: 95.50%\n",
      "Epoch 60/60\n",
      "Train Loss: 0.1306, Train Acc: 95.51%\n",
      "Val Loss: 0.3712, Val Acc: 87.64%\n",
      "Loaded best model from epoch 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/135285962.py:284: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.3928, Test Accuracy: 88.12%\n",
      "Final test accuracy: 88.12%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"Set seed for reproducibility\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "class StandardAlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(StandardAlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # First convolutional block\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Second convolutional block\n",
    "            nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Third convolutional block\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Fourth convolutional block\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Fifth convolutional block\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256 * 4 * 4, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "        \n",
    "        # Weight initialization\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Data transformations with basic augmentations\n",
    "        self.setup_data_transforms()\n",
    "        \n",
    "        # Load and split dataset\n",
    "        self.load_dataset()\n",
    "        \n",
    "        # Create model, optimizer, and loss function\n",
    "        self.model = StandardAlexNet().to(self.device)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.SGD(\n",
    "            self.model.parameters(), \n",
    "            lr=config.learning_rate, \n",
    "            momentum=config.momentum, \n",
    "            weight_decay=config.weight_decay\n",
    "        )\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        self.scheduler = optim.lr_scheduler.StepLR(\n",
    "            self.optimizer, \n",
    "            step_size=30, \n",
    "            gamma=0.1\n",
    "        )\n",
    "        \n",
    "    def setup_data_transforms(self):\n",
    "        # Basic data augmentation for training\n",
    "        self.transform_train = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "        ])\n",
    "        \n",
    "        # Test transforms\n",
    "        self.transform_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "        ])\n",
    "    \n",
    "    def load_dataset(self):\n",
    "        try:\n",
    "            dataset_path = os.path.join(self.config.input_path, \"cifar-10\")\n",
    "            if os.path.exists(dataset_path):\n",
    "                print(f\"Loading CIFAR-10 from dataset at {dataset_path}\")\n",
    "                full_trainset = torchvision.datasets.CIFAR10(\n",
    "                    root=dataset_path, train=True, download=False, transform=self.transform_train\n",
    "                )\n",
    "                testset = torchvision.datasets.CIFAR10(\n",
    "                    root=dataset_path, train=False, download=False, transform=self.transform_test\n",
    "                )\n",
    "            else:\n",
    "                print(\"CIFAR-10 dataset not found, downloading to working directory\")\n",
    "                full_trainset = torchvision.datasets.CIFAR10(\n",
    "                    root=self.config.output_path, train=True, download=True, transform=self.transform_train\n",
    "                )\n",
    "                testset = torchvision.datasets.CIFAR10(\n",
    "                    root=self.config.output_path, train=False, download=True, transform=self.transform_test\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading dataset: {e}\")\n",
    "            print(\"Falling back to downloading the dataset\")\n",
    "            full_trainset = torchvision.datasets.CIFAR10(\n",
    "                root=self.config.output_path, train=True, download=True, transform=self.transform_train\n",
    "            )\n",
    "            testset = torchvision.datasets.CIFAR10(\n",
    "                root=self.config.output_path, train=False, download=True, transform=self.transform_test\n",
    "            )\n",
    "        \n",
    "        val_size = int(len(full_trainset) * self.config.val_split)\n",
    "        train_size = len(full_trainset) - val_size\n",
    "        \n",
    "        trainset, valset = random_split(\n",
    "            full_trainset, [train_size, val_size],\n",
    "            generator=torch.Generator().manual_seed(self.config.seed)\n",
    "        )\n",
    "        \n",
    "        self.trainloader = DataLoader(\n",
    "            trainset, batch_size=self.config.batch_size, shuffle=True, \n",
    "            num_workers=self.config.num_workers, pin_memory=True\n",
    "        )\n",
    "        \n",
    "        self.valloader = DataLoader(\n",
    "            valset, batch_size=self.config.batch_size, shuffle=False, \n",
    "            num_workers=self.config.num_workers, pin_memory=True\n",
    "        )\n",
    "        \n",
    "        self.testloader = DataLoader(\n",
    "            testset, batch_size=self.config.batch_size, shuffle=False, \n",
    "            num_workers=self.config.num_workers, pin_memory=True\n",
    "        )\n",
    "        \n",
    "        self.classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "    \n",
    "    def train_epoch(self, epoch):\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(self.trainloader):\n",
    "            inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self.criterion(outputs, targets)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Update metrics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            # Print progress\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f\"Epoch: {epoch+1} | Batch: {batch_idx}/{len(self.trainloader)} | \"\n",
    "                      f\"Loss: {running_loss/(batch_idx+1):.4f} | \"\n",
    "                      f\"Acc: {100.*correct/total:.2f}%\")\n",
    "        \n",
    "        train_loss = running_loss / len(self.trainloader)\n",
    "        train_acc = 100. * correct / total\n",
    "        return train_loss, train_acc\n",
    "    \n",
    "    def evaluate(self, dataloader):\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in dataloader:\n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "                \n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        eval_loss = running_loss / len(dataloader)\n",
    "        eval_acc = 100. * correct / total\n",
    "        return eval_loss, eval_acc\n",
    "    \n",
    "    def save_checkpoint(self, epoch, val_loss, val_acc, filename):\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'val_acc': val_acc\n",
    "        }\n",
    "        torch.save(checkpoint, filename)\n",
    "    \n",
    "    def train(self):\n",
    "        print(f\"Starting training for {self.config.num_epochs} epochs\")\n",
    "        best_val_loss = float('inf')\n",
    "        \n",
    "        for epoch in range(self.config.num_epochs):\n",
    "            train_loss, train_acc = self.train_epoch(epoch)\n",
    "            \n",
    "            val_loss, val_acc = self.evaluate(self.valloader)\n",
    "            \n",
    "            self.scheduler.step()\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{self.config.num_epochs}\")\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "            \n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                print(f\"Saving best model with validation loss: {val_loss:.4f}\")\n",
    "                self.save_checkpoint(\n",
    "                    epoch, val_loss, val_acc, \n",
    "                    os.path.join(self.config.output_path, \"alexnet_cifar10_best.pt\")\n",
    "                )\n",
    "        \n",
    "        self.save_checkpoint(\n",
    "            epoch, val_loss, val_acc, \n",
    "            os.path.join(self.config.output_path, \"alexnet_cifar10_final.pt\")\n",
    "        )\n",
    "        \n",
    "        return best_val_loss, val_acc\n",
    "    \n",
    "    def evaluate_model(self):\n",
    "        checkpoint_path = os.path.join(self.config.output_path, \"alexnet_cifar10_best.pt\")\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"Loaded best model from epoch {checkpoint['epoch']+1}\")\n",
    "        \n",
    "        test_loss, test_acc = self.evaluate(self.testloader)\n",
    "        print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2f}%\")\n",
    "        \n",
    "        return test_acc\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.seed = 42\n",
    "        self.input_path = \"./data\"\n",
    "        self.output_path = \"./output\"\n",
    "        self.num_workers = 4\n",
    "        self.batch_size = 128\n",
    "        self.val_split = 0.1  # 10% of training data for validation\n",
    "        self.learning_rate = 0.01\n",
    "        self.weight_decay = 5e-4\n",
    "        self.momentum = 0.9\n",
    "        self.num_epochs = 60\n",
    "\n",
    "def main():\n",
    "    config = Config()\n",
    "    os.makedirs(config.output_path, exist_ok=True)\n",
    "    set_seed(config.seed)\n",
    "    trainer = Trainer(config)\n",
    "    trainer.train()\n",
    "    test_acc = trainer.evaluate_model()\n",
    "    print(f\"Final test accuracy: {test_acc:.2f}%\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
